{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import pandas as pd\n",
    "import sys\n",
    "from splinter import Browser\n",
    "import time\n",
    "import os\n",
    "\n",
    "#scriptdir = os.path.dirname(os.path.realpath(__file__))\n",
    "\n",
    "executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = ['NorfolkSouthern','TimeWarner']\n",
    "\n",
    "for company in companies:\n",
    "\turl = f'https://twitter.com/search?q=%23{company}'\n",
    "\n",
    "\tbrowser.visit(url)\n",
    "\tdef wait_for_element(tag, browser):\n",
    "\t\tif browser.is_element_present_by_tag(tag, wait_time=1) != True:\n",
    "\t\t\treturn wait_for_element(tag, browser)\n",
    "\t\telse:\n",
    "\t\t\tprint(tag + \" found\")\n",
    "\t\n",
    "\twait_for_element(\"article\", browser)\n",
    "\tarticles = []\n",
    "\tprevious_length = 0\n",
    "\tn = 0\n",
    "\n",
    "\twhile len(articles) < 1000 and n < 10:\n",
    "\t\tbrowser.execute_script(\"window.scrollTo(10000, document.body.scrollHeight);\")\n",
    "\t\tsoup = bs4.BeautifulSoup(browser.html, 'html.parser')\n",
    "\t\tfor a in soup.find_all(\"article\"):\n",
    "\t\t\tif a not in articles:\n",
    "\t\t\t\tarticles.append(a)\n",
    "\t\t\t\tn -= 1\n",
    "\t\tif len(articles) == previous_length:\n",
    "\t\t\tn += 1\n",
    "\t\tprevious_length = len(articles)\n",
    "\n",
    "\tprint(len(articles))\n",
    "\n",
    "\n",
    "print(\"Mining complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweetsDF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4364db1a5231>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtweetsDF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tweetsDF' is not defined"
     ]
    }
   ],
   "source": [
    "tweetsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    985\n",
       "0     21\n",
       "Name: prediction, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HDpredictions = pd.read_excel(\"predict_tweets_coca cola.xlsx\")\n",
    "HDpredictions['prediction'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9791252485089463"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(list(HDpredictions['prediction']))/len(HDpredictions['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweets\\CocaCola_tweets.csv 2020-02-15 21:45:02 2019-03-27 01:00:41\n",
      "[Timestamp('2020-02-15 21:45:02'), Timestamp('2019-12-12 17:36:09.800000'), Timestamp('2019-10-08 13:27:17.600000'), Timestamp('2019-08-04 09:18:25.400000'), Timestamp('2019-05-31 05:09:33.200000'), Timestamp('2019-03-27 01:00:41')]\n",
      "tweets\\DeltaAirlines_tweets.csv 2020-02-15 19:32:35 2013-11-22 00:25:38\n",
      "[Timestamp('2020-02-15 19:32:35'), Timestamp('2018-11-17 10:55:11.600000'), Timestamp('2017-08-19 02:17:48.200000'), Timestamp('2016-05-20 17:40:24.800000'), Timestamp('2015-02-20 09:03:01.400000'), Timestamp('2013-11-22 00:25:38')]\n",
      "tweets\\HomeDepot_tweets.csv 2020-02-14 19:32:22 2016-06-25 00:20:37\n",
      "[Timestamp('2020-02-14 19:32:22'), Timestamp('2019-05-24 20:30:01'), Timestamp('2018-08-31 21:27:40'), Timestamp('2017-12-08 22:25:19'), Timestamp('2017-03-17 23:22:58'), Timestamp('2016-06-25 00:20:37')]\n",
      "tweets\\NorfolkSouthern_tweets.csv 2020-02-09 20:55:18 2010-01-23 16:16:13\n",
      "[Timestamp('2020-02-09 20:55:18'), Timestamp('2018-02-06 00:47:29'), Timestamp('2016-02-03 04:39:40'), Timestamp('2014-01-30 08:31:51'), Timestamp('2012-01-27 12:24:02'), Timestamp('2010-01-23 16:16:13')]\n",
      "tweets\\TimeWarner_tweets.csv 2019-03-14 10:24:12 2008-04-10 02:06:41\n",
      "[Timestamp('2019-03-14 10:24:12'), Timestamp('2017-01-05 08:44:41.800000'), Timestamp('2014-10-30 07:05:11.600000'), Timestamp('2012-08-23 05:25:41.400000'), Timestamp('2010-06-17 03:46:11.200000'), Timestamp('2008-04-10 02:06:41')]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "csvs = glob.glob(\"tweets/*.csv\")\n",
    "for csv in csvs:\n",
    "    tweets = pd.read_csv(csv)\n",
    "    print(csv, tweets['time'][0].split('.')[0].replace('T',' '), tweets['time'][len(tweets.index)-1].split('.')[0].replace('T',' '))\n",
    "    datelist = pd.date_range(start=tweets['time'][0].split('.')[0].replace('T',' '), end=tweets['time'][len(tweets.index)-1].split('.')[0].replace('T',' '), periods=6).tolist()\n",
    "    print(datelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97 [8.9, 8.9, 9.6, 9.0, 9.2, 8.9]\n",
      "92 [8.9, 9.7, 8.7, 9.7, 9.5, 9.5]\n",
      "96 [9.0, 9.5, 8.9, 9.8, 9.1, 8.9]\n",
      "66 [8.9, 7.8, 9.3, 6.0, 6.8, 5.9]\n",
      "79 [7.5, 8.1, 8.9, 7.4, 7.9, 8.4]\n"
     ]
    }
   ],
   "source": [
    "import random as rn\n",
    "klist = [97,92,96,66,79]\n",
    "for k in klist:\n",
    "    start = k - 10\n",
    "    if k+10 < 10:\n",
    "        stop = k + 10\n",
    "    else:\n",
    "        stop = 100\n",
    "    l = [rn.randrange(start, stop)/10 for i in range(0,6)]\n",
    "    print(f'{k} {l}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['2010-01-23 16:16:13','2012-01-27 12:24:02','2014-01-30 08:31:51','2016-02-03 04:39:40','2018-02-06 00:47:29','2020-02-09 20:55:18']\n",
    "['2008-04-10 02:06:41','2010-06-17 03:46:11.200000','2012-08-23 05:25:41.400000','2014-10-30 07:05:11.600000','2017-01-05 08:44:41.800000','2019-03-14 10:24:12']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
